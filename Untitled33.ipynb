{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "18ymwmvwRU-kLgIp1x6cDCOLZeEaVEtgl",
      "authorship_tag": "ABX9TyNYmwnhA6kgyVkT8pi4YdlN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanUsw/Project-Overactive-Mind/blob/main/Untitled33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install higher for meta-learning support\n",
        "!pip install higher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-QvT3Q3CLAy",
        "outputId": "d9df4d5d-c5ba-430b-e84c-bf0372380e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->higher) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->higher) (1.3.0)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import higher  # For meta-learning algorithms\n"
      ],
      "metadata": {
        "id": "70qX6WRACQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "ds7Dp2OMCSPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageModule, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        # Fully connected layer for feature embedding\n",
        "        self.fc1 = nn.Linear(in_features=64 * 5 * 5, out_features=128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first convolution and activation\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # Apply max pooling\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Apply second convolution and activation\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # Apply max pooling\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        # Obtain feature embeddings\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0-unZmzxCUSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextModule(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(TextModule, self).__init__()\n",
        "        # Embedding layer to convert words to vectors\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        # LSTM layer for sequential processing\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        # Fully connected layer for feature embedding\n",
        "        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert word indices to embeddings\n",
        "        x = self.embedding(x)\n",
        "        # Obtain LSTM outputs and hidden states\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn is the hidden state from the last time step\n",
        "        hn = hn.squeeze(0)  # Remove unnecessary dimensions\n",
        "        # Obtain feature embeddings\n",
        "        x = self.fc1(hn)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "I11kxIrkCYCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageAutoencoder, self).__init__()\n",
        "        # Encoder (using the ImageModule)\n",
        "        self.encoder = ImageModule()\n",
        "        # Decoder layers to reconstruct the image\n",
        "        self.decoder_fc = nn.Linear(in_features=128, out_features=64 * 5 * 5)\n",
        "        self.decoder_conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3)\n",
        "        self.decoder_conv2 = nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode the image to get features\n",
        "        x = self.encoder(x)\n",
        "        # Decode features back to image shape\n",
        "        x = F.relu(self.decoder_fc(x))\n",
        "        x = x.view(-1, 64, 5, 5)\n",
        "        x = F.relu(self.decoder_conv1(x))\n",
        "        x = torch.sigmoid(self.decoder_conv2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lHJ6MUqWCY-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "u1KyKWqeCcuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the autoencoder and move it to the device\n",
        "autoencoder = ImageAutoencoder().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5  # You can increase this for better results\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images, _ in train_loader:\n",
        "        # Move images to the device\n",
        "        images = images.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: reconstruct images\n",
        "        outputs = autoencoder(images)\n",
        "        # Compute reconstruction loss\n",
        "        loss = criterion(outputs, images)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "_LY-LNqmCfIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_task_data(digit):\n",
        "    # Filter dataset for a specific digit\n",
        "    indices = (train_dataset.targets == digit).nonzero().flatten()\n",
        "    subset = torch.utils.data.Subset(train_dataset, indices)\n",
        "    loader = DataLoader(subset, batch_size=5, shuffle=True)  # 5-shot learning\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "pmDL5YYKCg1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "meta_model = ImageModule().to(device)\n",
        "\n",
        "# Meta-optimizer for the meta-model\n",
        "meta_optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function\n",
        "meta_criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "9SJFjxBICi0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_meta_epochs = 1  # Increase as needed\n",
        "\n",
        "for epoch in range(num_meta_epochs):\n",
        "    total_meta_loss = 0\n",
        "    # Iterate over different tasks (digits 0-9)\n",
        "    for digit in range(10):\n",
        "        # Get task-specific data loader\n",
        "        task_loader = get_task_data(digit)\n",
        "        # Get one batch of data\n",
        "        images, _ = next(iter(task_loader))\n",
        "        images = images.to(device)\n",
        "        labels = torch.zeros(images.size(0), dtype=torch.long).to(device)  # All labels are 0 for this task\n",
        "        # Create a fast adapter for the model\n",
        "        with higher.innerloop_ctx(meta_model, meta_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
        "            # Inner loop (task-specific adaptation)\n",
        "            for _ in range(1):  # One step adaptation\n",
        "                outputs = fmodel(images)\n",
        "                loss = meta_criterion(outputs, labels)\n",
        "                diffopt.step(loss)\n",
        "            # Compute meta-loss (using the same data for simplicity)\n",
        "            outputs = fmodel(images)\n",
        "            meta_loss = meta_criterion(outputs, labels)\n",
        "            # Accumulate meta-gradient\n",
        "            meta_loss.backward()\n",
        "            total_meta_loss += meta_loss.item()\n",
        "    # Meta-optimization step\n",
        "    meta_optimizer.step()\n",
        "    meta_optimizer.zero_grad()\n",
        "    avg_meta_loss = total_meta_loss / 10  # Average over tasks\n",
        "    print(f'Meta Epoch [{epoch+1}/{num_meta_epochs}], Meta Loss: {avg_meta_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "mNGkfhx7ClZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(HierarchicalModel, self).__init__()\n",
        "        # Initialize the specialized modules\n",
        "        self.image_module = ImageModule()\n",
        "        self.text_module = TextModule(vocab_size, embedding_dim, hidden_dim)\n",
        "        # Final classifier that combines features\n",
        "        self.fc = nn.Linear(in_features=128 * 2, out_features=10)  # Assuming 10 classes\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        # Extract features from image\n",
        "        image_features = self.image_module(image)\n",
        "        # Extract features from text\n",
        "        text_features = self.text_module(text)\n",
        "        # Concatenate features\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "        # Classification output\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "b_7J_sGhCrFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary size and dimensions\n",
        "vocab_size = 1000  # Size of the vocabulary\n",
        "embedding_dim = 50\n",
        "hidden_dim = 64\n",
        "\n",
        "# Create dummy text input (batch_size x sequence_length)\n",
        "text_input = torch.randint(0, vocab_size, (64, 10)).to(device)  # Batch of 64 sequences, each of length 10\n"
      ],
      "metadata": {
        "id": "91IBX6vMCuVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the hierarchical model\n",
        "hierarchical_model = HierarchicalModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "# Get a batch of images\n",
        "images, _ = next(iter(train_loader))\n",
        "images = images.to(device)\n",
        "\n",
        "# Forward pass through the hierarchical model\n",
        "outputs = hierarchical_model(images, text_input)\n",
        "print(f'Output shape: {outputs.shape}')  # Should be (batch_size, 10)\n"
      ],
      "metadata": {
        "id": "1hy0DYWfCvrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}